# Week 1 Notes

- Key questions along this week
    - What is a transformer architecture?
    - What is multi-headed attention mechanism? What’s the intuition behind using it?
    - Why will transformer architecture take off to work?
        - allow attention to work in a massive parallel way - makes it work on modern GPUs and can scale up.
    - How the text focus(LLM) basic transformer laid a solid foundation for vision transformer as well?
    - What is Gen AI project lifecycle? How to think about when developing your AI app?
        - which model to choose? off-the-shelf one or self pre-trained one? what’s the proper model size?
- Gen AI & LLM use cases & Modle lifecycle
    - Gen AI is a subset of traditional ML. The ML models that underpin gen AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans.
    - Foundation models (size relative to its parameter size)
        
        ![image.png](Week%201%20Notes/image.png)
        
    - Prompt &  completions & context window & inference
        - context window: space or memory available to the prompt
        - inference: the act of using the model to generate text
        - completion: text contained in the original prompt followed by the generated text
        
        ![image.png](Week%201%20Notes/image%201.png)
        
- LLM Use cases
    - chatbots, write an essay, summarize conversation, translation, code
    - information retrieval: e.g. named entity recognition - a word classification
    - augmenting LLMs by connecting them to external data sources or use them to invoke external APIs.
        - use this ability to provide model with info it doesn’t know from its pre-training and enable the model to power interactions with the real-world. (week3)
    - Increase parameter amount → increase language understanding. But small models can also be fine-tuned for specific tasks (week2)
- Text generation before transformers (with RNN)
    - recurrent neural networks, were limited by the amount of compute and memory needed to perform well.
        
        As you scale the RNN to see more preceding words in the text, you have to significantly scale the resources used.
        
        To successfully predict the next word, models need to see more than just previous few words. Models need to have an understanding of the whole sentence or even whole doc.
        
    - 2017 paper: Attention is all you need. (Google & UToronto)
        - scaled efficiently to use multi-core GPUs, parallel process input data (use much larger dataset), and crucially it’s able to learn to pay attention to the meaning of the words it’s processing.
- Transformer architecture
    - the power lies in its ability to learn the relevance and context of all of the words in a sentence, not just to each neighbor words, but to every other word in a sentence. And to apply attention weights to those relationships.
        
        ![image.png](Week%201%20Notes/image%202.png)
        
        Attention map below: useful to illustrate the attention weights between each word and every other word.
        
        ![image.png](Week%201%20Notes/image%203.png)
        
    - Structure Graph (Encoder and Decoder are very similar)
        
        ![image.png](Week%201%20Notes/image%204.png)
        
        ![image.png](Week%201%20Notes/image%205.png)
        
    - part1 - tokenize - convert words to numbers
        - each number representing a position in a dict of all the possible words that the model can work with.
        - multiple tokenization methods to choose. e.g. one ID for one word, or one ID for part of the word. What’s important is once you selected a tokenizer to train the model, you must use the same tokenizer when generate the text.
    - part2 - embedding layer - each token as a vector is a high-dim space
        - This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
        - the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence.  (In the original transformer paper, the vector size was 512)
            - Simple example in 3-dim
                
                ![image.png](Week%201%20Notes/image%206.png)
                
                for example, related words are located closer to each other in the embedding space. And you can calculate the distance between the words as an angle, which give the model the ability to mathematically understand language.
                
        - Position encoding - preserve the info about the word order
            
            Because the model processes each of the input tokens in parallel, by adding position encoding we don’t lose the relevance of the position of the word in the sentence.
            
        
        ![image.png](Week%201%20Notes/image%207.png)
        
    - part3 - self-attention layer - model analyze different relationships(multi-headed) between the tokens to better capture the contextual dependencies.
        - The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence.
        - multi-headed self-attention
            - This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other.
            - The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.
            - The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme.
            - It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.
                
                Some attention maps may be easy to interpret, others may not.
                
    - part4 - feed-forward network for output processing - apply attention weights to input data to have the output as a vector of logits proportional to the probability score of each and every token in the tokenized dict.
    - part5 - softmax layer - output vector normalized into a probability score for each word in the dict.
        
        The highest score one represents the most likely predicted token. But as you'll see later in the course, there are a number of methods that you can use to vary the final selection from this vector of probabilities.
        
        ![image.png](Week%201%20Notes/image%208.png)
        
- Generating text with transformers (example walkthrough - translation [sequence-to-sequence task])
    - Encoder part: tokenize the input → embedding the tokens → multi-headed self-attention of the tokens → apply attention weight to input via FFN to have encoder output
        
        At this point, the data that leaves the encoder is a deep representation of the structure and meaning of the input sequence. 
        
        This representation is inserted into the middle of the decoder to influence the decoder's self-attention mechanisms.
        
        ![image.png](Week%201%20Notes/image%209.png)
        
    - Decoder part: input (a start of sequence token) → embedding → based on the contextual understanding provided by encoder (attention layer) → apply attention output via decoder FFN → final softmax output layer to predict the next token → continue the loop by passing the output token back to the input to trigger the generation of the next token → until model predicts an end-of-sequence token.
        
        ![image.png](Week%201%20Notes/image%2010.png)
        
        Finally, detokenize the final token sequence into words.
        
        ![image.png](Week%201%20Notes/image%2011.png)
        
        There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative you are generated text is. You will look at these in more detail later this week. 
        
    - Summary
        
        The complete transformer architecture consists of an encoder and decoder components.
        
        - The encoder encodes input sequences into a deep representation of the structure and meaning of the input.
        - The decoder, working from input token triggers, uses the encoder's contextual understanding to generate new tokens. It does this in a loop until some stop condition has been reached.
        
        ![image.png](Week%201%20Notes/image%2012.png)
        
    - Encoders and Decoders can be separately used (3 types)
        - Encoder-only: work as sequence-to-sequence models, but without further modification, the input sequence and the output sequence or the same length.
            
            Their use is less common these days, but by adding additional layers to the architecture, you can train encoder-only models to perform classification tasks such as sentiment analysis, BERT is an example of an encoder-only model. 
            
        - Encoder-Decoder model: perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. You can also scale and train this type of model to perform general text generation tasks. Examples of encoder-decoder models include BART as opposed to BERT, and T5.
        - Decoder-only: most commonly used today. Again, as they have scaled, their capabilities have grown. These models can now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, LLaMA, and many more.
- Prompting and Prompt Engineering
    - prompt engineering: develop and improve the prompt to have better result
    - one powerful strategy is to include examples of the task that you want the model to carry out inside the prompt - known as in-context learning (ICL)
        - zero-shot (no example) / one shot (one example) / few shot (multiple examples)
            
            ![image.png](Week%201%20Notes/image%2013.png)
            
        - While the largest models are good at zero-shot inference with no examples, smaller models can benefit from one-shot or few-shot inference that include examples of the desired behavior.
            
            But remember the context window because you have a limit on the amount of in-context learning that you can pass into the model. Generally, if you find that your model isn't performing well when, say, including five or six examples, you should try fine-tuning your model instead.
            
        
    - You may have to try out a few models to find the right one for your use case. Once you've found the model that is working for you, there are a few settings that you can experiment with to influence the structure and style of the completions that the model generates. (Generative configuration part below)
- Generative configuration - parameter configs that can be used to influence the way the model makes the final decision about next-word generation.
    - Note that these are different than the training parameters which are learned during training time. Instead, these configuration parameters are invoked at inference time and give you control over things like the maximum number of tokens in the completion, and how creative the output is.
        
        ![image.png](Week%201%20Notes/image%2014.png)
        
    - max new tokens - limit the number of tokens that the model will generate. You can think of this as putting a cap on the number of times the model will go through the selection process.
    - sample top K and sample top P - ways on how to choose the next word based on the prob score vector
        - Most large language models by default will operate with so-called greedy decoding.
            
            This is the simplest form of next-word prediction, where the model will always choose the word with the highest probability. This method can work very well for short generation but is susceptible to repeated words or repeated sequences of words. 
            
        - Random sampling is the easiest way to introduce some variability - to generate text that's more natural, more creative and avoids repeating words.
            
            The model chooses an output word at random using the probability distribution to weight the selection.
            
            However, depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense.
            
        - top K and top P are used to limit the random sampling and increase the chance for sensible output.
            - top K - instructs the model to choose from only the k tokens with the highest probability.
                
                This method can help the model have some randomness while preventing the selection of highly improbable completion words. This in turn makes your text generation more likely to sound reasonable and to make sense
                
            - top P - instructs the model to choose the top predictions whose combined probability doesn’t exceed P.
    - Temperature - This parameter influences the shape of the probability distribution that the model calculates for the next token.
        
        Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness. 
        
        The temperature value is a scaling factor that's applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token. 
        
        In contrast to the top k and top p parameters, changing the temperature actually alters the predictions that the model will make. 
        
        ![image.png](Week%201%20Notes/image%2015.png)
        
        - If you choose a low value of temperature, say less than one, the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words. The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training.
        - If instead you set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token. This leads the model to generate text with a higher degree of randomness and more variability in the output compared to a cool temperature setting. This can help you generate text that sounds more creative.
        - If you leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used.
- Gen AI project lifecycle
    
    ![image.png](Week%201%20Notes/image%2016.png)
    
    - scope
        
        The most important step in any project is to define the scope as accurately and narrowly as you can.
        
        As you've seen in this course so far, LLMs are capable of carrying out many tasks, but their abilities depend strongly on the size and architecture of the model. You should think about what function the LLM will have in your specific application. 
        
        Getting really specific about what you need your model to do can save you time and perhaps more importantly, compute cost. 
        
    - Select
        
        Your first decision will be whether to train your own model from scratch or work with an existing base model. In general, you'll start with an existing model, although there are some cases where you may find it necessary to train a model from scratch. (details later)
        
    - Adapt and align model
        
        With your model in hand, the next step is to assess its performance and carry out additional training if needed for your application. 
        
        Prompt engineering can sometimes be enough to get your model to perform well, so you'll likely start by trying in-context learning, using examples suited to your task and use case. 
        
        There are still cases, however, where the model may not perform as well as you need, even with one or a few short inference, and in that case, you can try fine-tuning your model (supervised learning process, details later).
        
        As models become more capable, it's becoming increasingly important to ensure that they behave well and in a way that is aligned with human preferences in deployment. (reinforcement learning with human feedback, details later)
        
        Note that this adapt and aligned stage of app development can be highly iterative. You may start by trying prompt engineering and evaluating the outputs, then using fine tuning to improve performance and then revisiting and evaluating prompt engineering one more time to get the performance that you need. 
        
         
        
    - Application integration
        
        Finally, when you've got a model that is meeting your performance needs and is well aligned, you can deploy it into your infrastructure and integrate it with your application. At this stage, an important step is to optimize your model for deployment. 
        
        The last but very important step is to consider any additional infrastructure that your application will require to work well. There are some fundamental limitations of LLMs that can be difficult to overcome through training alone like their tendency to invent information when they don't know an answer, or their limited ability to carry out complex reasoning and mathematics. (powerful techniques details later)
        
    
- Lab1 Intro (Summarize a dialogue) - no access due to audit
    - use FLAN-T5 model, try zero-shot, one-shot, few-shot, and get an intuition on diff generation configs.
    - If you have no idea how a model is, if you just get it off of some model hub somewhere. These are the first step - Prompt engineering, zero-shot, one-shot, few shot is almost always the first step when you're trying to learn the language model that you've been handed and dataset. Also very datasets specific as well, and task-specific.
    - For few shots: typically if you can’t get idea result from 5-6 shots, it means the current model just can’t do it. Consider fine-tuning after this.
    - Lab notes from github repo (https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/blob/main/Week-1/Lab_1_summarize_dialogue.ipynb)
        - install the required packages to use PyTorch and Hugging Face transformers and datasets.
        - Load the datasets, Large Language Model (LLM), tokenizer, and configurator.
            
            ```python
            
            from datasets import load_dataset
            from transformers import AutoModelForSeq2SeqLM
            from transformers import AutoTokenizer
            from transformers import GenerationConfig
            ```
            
        - The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index).
            
            https://huggingface.co/docs/transformers/index
            
            https://huggingface.co/learn/nlp-course/chapter1/1
            
        - Load the FLAN-T5 model
            
            ```python
            model_name='google/flan-t5-base'
            
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            ```
            
        - Tokenize
            
            Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer).
            
            ```python
            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
            
            sentence = "What time is it, Tom?"
            
            sentence_encoded = tokenizer(sentence, return_tensors='pt')
            
            sentence_decoded = tokenizer.decode(
                    sentence_encoded["input_ids"][0], 
                    skip_special_tokens=True
                )
            
            print('ENCODED SENTENCE:')
            print(sentence_encoded["input_ids"][0])
            print('\nDECODED SENTENCE:')
            print(sentence_decoded)
            ```
            
        - Explore prompt engineering
            - without prompt engineering
                
                ```python
                for i, index in enumerate(example_indices):
                    dialogue = dataset['test'][index]['dialogue']
                    summary = dataset['test'][index]['summary']
                    
                    inputs = tokenizer(dialogue, return_tensors='pt')
                    output = tokenizer.decode(
                        model.generate(
                            inputs["input_ids"], 
                            max_new_tokens=50,
                        )[0], 
                        skip_special_tokens=True
                    )
                    
                    print(dash_line)
                    print('Example ', i + 1)
                    print(dash_line)
                    print(f'INPUT PROMPT:\n{dialogue}')
                    print(dash_line)
                    print(f'BASELINE HUMAN SUMMARY:\n{summary}')
                    print(dash_line)
                    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n{output}\n')
                ```
                
            - Zero Shot Inference with an Instruction Prompt
                
                ```python
                for i, index in enumerate(example_indices):
                    dialogue = dataset['test'][index]['dialogue']
                    summary = dataset['test'][index]['summary']
                
                    prompt = f"""
                Summarize the following conversation.
                
                {dialogue}
                
                Summary:
                    """
                
                    # Input constructed prompt instead of the dialogue.
                    inputs = tokenizer(prompt, return_tensors='pt')
                    output = tokenizer.decode(
                        model.generate(
                            inputs["input_ids"], 
                            max_new_tokens=50,
                        )[0], 
                        skip_special_tokens=True
                    )
                    
                    print(dash_line)
                    print('Example ', i + 1)
                    print(dash_line)
                    print(f'INPUT PROMPT:\n{prompt}')
                    print(dash_line)
                    print(f'BASELINE HUMAN SUMMARY:\n{summary}')
                    print(dash_line)    
                    print(f'MODEL GENERATION - ZERO SHOT:\n{output}\n')
                ```
                
                Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, you will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):
                
                ```python
                prompt = f"""
                Dialogue:
                
                {dialogue}
                
                What was going on?
                """
                
                    inputs = tokenizer(prompt, return_tensors='pt')
                ```
                
            - One Shot and few shot Inference
                
                Let's build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which you want the model to complete (`example_index_to_summarize`). 
                
                ```python
                def make_prompt(example_indices_full, example_index_to_summarize):
                    prompt = ''
                    for index in example_indices_full:
                        dialogue = dataset['test'][index]['dialogue']
                        summary = dataset['test'][index]['summary']
                        
                        # The stop sequence '{summary}\n\n\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.
                        prompt += f"""
                Dialogue:
                
                {dialogue}
                
                What was going on?
                {summary}
                
                """
                    
                    dialogue = dataset['test'][example_index_to_summarize]['dialogue']
                    
                    prompt += f"""
                Dialogue:
                
                {dialogue}
                
                What was going on?
                """
                        
                    return prompt
                ```
                
                ```python
                # One shot
                example_indices_full = [40]
                example_index_to_summarize = 200
                
                one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)
                
                print(one_shot_prompt)
                
                summary = dataset['test'][example_index_to_summarize]['summary']
                
                inputs = tokenizer(one_shot_prompt, return_tensors='pt')
                output = tokenizer.decode(
                    model.generate(
                        inputs["input_ids"],
                        max_new_tokens=50,
                    )[0], 
                    skip_special_tokens=True
                )
                
                print(dash_line)
                print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
                print(dash_line)
                print(f'MODEL GENERATION - ONE SHOT:\n{output}')
                
                # Few shot
                example_indices_full = [40, 80, 120]
                example_index_to_summarize = 200
                
                few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)
                
                summary = dataset['test'][example_index_to_summarize]['summary']
                
                inputs = tokenizer(few_shot_prompt, return_tensors='pt')
                output = tokenizer.decode(
                    model.generate(
                        inputs["input_ids"],
                        max_new_tokens=50,
                    )[0], 
                    skip_special_tokens=True
                )
                
                print(dash_line)
                print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
                print(dash_line)
                print(f'MODEL GENERATION - FEW SHOT:\n{output}')
                ```
                
                In this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored.
                
        - Changing configuration parameters for inference
            - You can change the configuration parameters of the `generate()` method to see a different output from the LLM. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).
            - A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`).
            
            ```python
            generation_config = GenerationConfig(max_new_tokens=50)
            # generation_config = GenerationConfig(max_new_tokens=10)
            # generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)
            # generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)
            # generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)
            
            inputs = tokenizer(few_shot_prompt, return_tensors='pt')
            output = tokenizer.decode(
                model.generate(
                    inputs["input_ids"],
                    generation_config=generation_config,
                )[0], 
                skip_special_tokens=True
            )
            
            print(dash_line)
            print(f'MODEL GENERATION - FEW SHOT:\n{output}')
            print(dash_line)
            print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
            
            ```
            

---

- Pre-training LLM
    - choosing models: The developers of some of the major frameworks for building generative AI applications like Hugging Face and PyTorch, have curated hubs where you can browse these models. A really useful feature of these hubs is the inclusion of model cards, that describe important details including the best use cases for each model, how it was trained, and known limitations.
        
        ![image.png](Week%201%20Notes/image%2017.png)
        
    - First understand how the model is trained to help with the choice
    - LLM pre-training at a high level
        
        ![image.png](Week%201%20Notes/image%2018.png)
        
        pre-training phase when the model learns from vast amounts of unstructured textual data empowers LLM to encode a deep statistical representation of language.
        
        In this self-supervised learning step, the model internalizes the patterns and structures present in the language. These patterns then enable the model to complete its training objective, which depends on the architecture of the model. During pre-training, the model weights get updated to minimize the loss of the training objective. 
        
        Pre-training also requires a large amount of compute and the use of GPUs.
        
        As a result of this data quality curation, often only 1-3% of tokens are used for pre-training. You should consider this when you estimate how much data you need to collect if you decide to pre-train your own model. 
        
    - Three variance of the transformer model: encoder-only, decode-only and encoder-decoder. Each of these is trained on a different objective.
        - Encoder-only: a.k.a. Autoencoding models
            
            ![image.png](Week%201%20Notes/image%2019.png)
            
            - pretrained using masked language modeling
                
                tokens in the input sequence or randomly mask, and the training objective is to predict the mask tokens in order to reconstruct the original sentence. This is also called a denoising objective. 
                
            - Autoencoding models spilled bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before.
            - Encoder-only models are ideally suited to task that benefit from this bi-directional contexts.
            
            ![image.png](Week%201%20Notes/image%2020.png)
            
        - Decoder-only: a.k.a. Autoregressive model
            
            ![image.png](Week%201%20Notes/image%2021.png)
            
            - pre-trained using causal language modeling. Here, the training objective is to predict the next token based on the previous sequence of tokens. Predicting the next token is sometimes called full language modeling by researchers.
            - Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question. The model has no knowledge of the end of the sentence. The model then iterates over the input sequence one by one to predict the following token. In contrast to the encoder architecture, this means that the context is unidirectional.
            - Decoder-only models are often used for text generation, although larger decoder-only models show strong zero-shot inference abilities, and can often perform a range of tasks well.
                
                ![image.png](Week%201%20Notes/image%2022.png)
                
        - Encoder-Decoder: sequence-to-sequence model
            - The exact details of the pre-training objective vary from model to model.
                
                A popular sequence-to-sequence model T5, pre-trains the encoder using span corruption, which masks random sequences of input tokens. Those mass sequences are then replaced with a unique Sentinel token, shown here as x. Sentinel tokens are special tokens added to the vocabulary, but do not correspond to any actual word from the input text. 
                
                The decoder is then tasked with reconstructing the mask token sequences auto-regressively. The output is the Sentinel token followed by the predicted tokens. 
                
                ![image.png](Week%201%20Notes/image%2023.png)
                
            - They are generally useful in cases where you have a body of texts as both input and output.
                
                ![image.png](Week%201%20Notes/image%2024.png)
                
        
        ![image.png](Week%201%20Notes/image%2025.png)
        
    - always larger model? - training can be more challenge
        
        ![image.png](Week%201%20Notes/image%2026.png)
        
        Researchers have found that the larger a model, the more likely it is to work as you needed to without additional in-context learning or further training. This observed trend of increased model capability with size has driven the development of larger and larger models in recent years. This growth has been fueled by inflection points and research, such as the introduction of the highly scalable transformer architecture, access to massive amounts of data for training, and the development of more powerful compute resources. 
        
- Computational challenges of training LLMs
    
    Running out of memory issue
    
    ![image.png](Week%201%20Notes/image%2027.png)
    
    If you've ever tried training or even just loading your model on Nvidia GPUs, this error message might look familiar. CUDA, short for Compute Unified Device Architecture, is a collection of libraries and tools developed for Nvidia GPUs. Libraries such as PyTorch and TensorFlow use CUDA to boost performance on metrics multiplication and other operations common to deep learning.
    
    - Memory size approximation
        
        ![image.png](Week%201%20Notes/image%2028.png)
        
        ![image.png](Week%201%20Notes/image%2029.png)
        
        ![image.png](Week%201%20Notes/image%2030.png)
        
        This is definitely too large for consumer hardware, and even challenging for hardware used in data centers, if you want to train with a single processor.
        
    - options to mitigate: Quantization - reduce precision
        
        reduce the memory required to store the weights of your model by reducing their precision from 32-bit floating point numbers to 16-bit floating point numbers, or eight-bit integer numbers.
        
        Quantization statistically projects the original 32-bit floating point numbers into a lower precision space, using scaling factors calculated based on the range of the original 32-bit floating point numbers. 
        
        ![image.png](Week%201%20Notes/image%2031.png)
        
        ![image.png](Week%201%20Notes/image%2032.png)
        
        The AI research community has explored ways to optimize16-bit quantization. One datatype in particular BFLOAT16, has recently become a popular alternative to FP16. 
        
        ![image.png](Week%201%20Notes/image%2033.png)
        
        BF16 significantly helps with training stability and is supported by newer GPU's such as NVIDIA's A100. BFLOAT16 is often described as a truncated 32-bit float, as it captures the full dynamic range of the full 32-bit float, that uses only 16-bits. BFLOAT16 uses the full eight bits to represent the exponent, but truncates the fraction to just seven bits. This not only saves memory, but also increases model performance by speeding up calculations. The downside is that BF16 is not well suited for integer calculations, but these are relatively rare in deep learning.
        
        ![image.png](Week%201%20Notes/image%2034.png)
        
        Remember that the goal of quantization is to reduce the memory required to store and train models by reducing the precision off the model weights. Quantization statistically projects the original 32-bit floating point numbers into lower precision spaces using scaling factors calculated based on the range of the original 32-bit floats. Modern deep learning frameworks and libraries support quantization-aware training, which learns the quantization scaling factors during the training process. 
        
    - Model can have too many parameters that only rely on quantization can’t solve it - train your model across multiple GPUs
        
        ![image.png](Week%201%20Notes/image%2035.png)
        
        As modal scale beyond a few billion parameters, it becomes impossible to train them on a single GPU. Instead, you'll need to turn to distributed computing techniques while you train your model across multiple GPUs. This could require access to hundreds of GPUs, which is very expensive. Another reason why you won't pre-train your own model from scratch most of the time. 
        
        However, an additional training process called fine-tuning, which also requires storing all training parameters in memory and it's very likely you'll want to fine tune a model at some point. Next part will help you understand more about the technical aspects of training across GPUs.
        
- Efficient multi-GPU compute startegies
    - even if your model does fit onto a single GPU, there are benefits to using multiple GPUs to speed up your training.
    - Base case: your model still fits on a single GPU
        - The first step in scaling model training is to distribute large data-sets across multiple GPUs and process these batches of data in parallel.
        - Popular implementation: Pytorch’s distributed data-parallel (DDP)
            
            ![image.png](Week%201%20Notes/image%2036.png)
            
            DDP copies your model onto each GPU and sends batches of data to each of the GPUs in parallel. Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips. 
            
            Note that DDP requires that your model weights and all of the additional parameters, gradients, and optimizer states that are needed for training, fit onto a single GPU. 
            
    - Bigger case: your model is too large to fit on a single GPU
        - tech called: model sharding
        - popular implementation: Pytorch’s fully sharded data parallel (FSDP)
            
            ![image.png](Week%201%20Notes/image%2037.png)
            
            ZeRO stands for zero redundancy optimizer and the goal of ZeRO is to optimize memory by distributing or sharding model states across GPUs with zero data overlap. 
            
        - How ZeRO works
            
            ![image.png](Week%201%20Notes/image%2038.png)
            
            the largest memory requirement was for the optimizer states, which take up twice as much space as the weights, followed by weights themselves and the gradients. 
            
            In DDP, we keep a full model copy on each GPU, which leads to redundant memory consumption. 
            
            ZeRO, on the other hand, eliminates this redundancy by sharding the model parameters, gradients, and optimizer states across GPUs instead of replicating them.  At the same time, the communication overhead for a sinking model states stays close to that of the DDP. 
            
            ![image.png](Week%201%20Notes/image%2039.png)
            
            ZeRO offers three optimization stages. 
            
            - ZeRO Stage 1, shots only optimizer states across GPUs, this can reduce your memory footprint by up to a factor of four. Z
            - eRO Stage 2 also shots the gradients across chips. When applied together with Stage 1, this can reduce your memory footprint by up to eight times.
            - Finally, ZeRO Stage 3 shots all components including the model parameters across GPUs. When applied together with Stages 1 and 2, memory reduction is linear with a number of GPUs. For example, sharding across 64 GPUs could reduce your memory by a factor of 64.
        - How FSDP works
            
            ![image.png](Week%201%20Notes/image%2040.png)
            
            - FSDP requires you to collect this data from all of the GPUs before the forward and backward pass. Each GPU requests data from the other GPUs on-demand to materialize the sharded data into unsharded data for the duration of the operation.
            - After the operation, you release the unsharded non-local data back to the other GPUs as original sharded data. You can also choose to keep it for future operations during backward pass for example.
            - Note, this requires more GPU RAM again (bcs more communications between GPUs are required), this is a typical performance versus memory trade-off decision.
            - In the final step after the backward pass, FSDP is synchronizes the gradients across the GPUs in the same way they were for DDP.
            - To manage the trade-off between performance and memory utilization, you can configure the level of sharding using FSDP is sharding factor.
                
                ![image.png](Week%201%20Notes/image%2041.png)
                
- Scaling laws and compute-optimal models
    
    learn about research that has explored the relationship between model size, training, configuration and performance in an effort to determine just how big models need to be. 
    
    ![image.png](Week%201%20Notes/image%2042.png)
    
    - A petaFLOP per second day - define a unit of compute that quantifies the required resources.
        
        ![image.png](Week%201%20Notes/image%2043.png)
        
        ![image.png](Week%201%20Notes/image%2044.png)
        
        ![image.png](Week%201%20Notes/image%2045.png)
        
    - well-defined relationships between these three scaling choices
        
        In practice however, the compute resources you have available for training will generally be a hard constraint set by factors such as the hardware you have access to, the time available for training and the financial budget of the project. If you hold your compute budget fixed, the two levers you have to improve your model's performance are the size of the training dataset and the number of parameters in your model. 
        
        ![image.png](Week%201%20Notes/image%2046.png)
        
        The OpenAI researchers found that these two quantities also show a power-law relationship with a test loss in the case where the other two variables are held fixed. 
        
        ![image.png](Week%201%20Notes/image%2047.png)
        
    - what's the ideal balance between these three quantities?
        - the Chinchilla paper - 2022 research paper - The goal was to find the optimal number of parameters and volume of training data for a given compute budget.
        - The Chinchilla paper hints that many of the 100 billion parameter large language models like GPT-3 may actually be over parameterized, meaning they have more parameters than they need to achieve a good understanding of language and under trained so that they would benefit from seeing more training data.
        - The authors hypothesized that smaller models may be able to achieve the same performance as much larger ones if they are trained on larger datasets.
        - One important takeaway from the Chinchilla paper is that the optimal training dataset size for a given model is about 20 times larger than the number of parameters in the model. Chinchilla was determined to be compute optimal.
        - Another important result from the paper is that the compute optimal Chinchilla model outperforms non compute optimal models such as GPT-3 on a large range of downstream evaluation tasks.
        - With the results of the Chinchilla paper in hand teams have recently started to develop smaller models that achieved similar, if not better results than larger models that were trained in a non-optimal way.
        
        ![image.png](Week%201%20Notes/image%2048.png)
        
        - Bloomberg GPT, is a really interesting model. It was trained in a compute optimal way following the Chinchilla loss and so achieves good performance with the size of 50 billion parameters. It's also an interesting example of a situation where pre-training a model from scratch was necessary to achieve good task performance.
- BloombergGPT - Pre-training for domain adaptation
    - Generally it’s recommended to work with an existing LLM as you develop your application. However, if your target domain uses vocabulary and language structures that are not commonly used in day to day language (e.g. Finance, Law, Medicine), you may need to perform domain adaptation to achieve good model performance.
    - BloombergGPT, first announced in 2023 in a paper by Shijie Wu.
        - Pre-trained with finance data
        - The goal is to achieve best results on financial benchmarks while also maintaining competitive performance on general purpose LLM benchmarks. As such, the researchers chose data consisting of 51% financial data and 49% public data.
            
            ![image.png](Week%201%20Notes/image%2049.png)
            
        - They also discuss how they started with a chinchilla scaling laws for guidance and where they had to make tradeoffs.
            
            ![image.png](Week%201%20Notes/image%2050.png)
            
            The dashed pink line on each graph indicates the compute budget that the Bloomberg team had available for training their new model. The pink shaded regions correspond to the compute optimal scaling loss determined in the Chinchilla paper. 
            
            - The number of parameters is fairly close to optimal.
            - The smaller than optimal training data set is due to the limited availability of financial domain data. Showing that real world constraints may force you to make trade offs when pretraining your own models.